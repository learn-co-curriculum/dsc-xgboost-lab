{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# XGBoost - Lab\n", "\n", "## Introduction\n", "\n", "In this lab, we'll install the popular [XGBoost](http://xgboost.readthedocs.io/en/latest/index.html) library and explore how to use this popular boosting model to classify different types of wine using the [Wine Quality Dataset](https://archive.ics.uci.edu/ml/datasets/wine+quality) from the UCI Machine Learning Dataset Repository.  \n", "\n", "## Objectives\n", "\n", "You will be able to:\n", "\n", "- Fit, tune, and evaluate an XGBoost algorithm\n", "\n", "## Installing XGBoost\n", "\n", "The XGBoost model is not currently included in scikit-learn, so we'll have to install it on our own.  To install XGBoost, you'll need to use `conda`. \n", "\n", "To install XGBoost, follow these steps:\n", "\n", "1. Open up a new terminal window \n", "2. Activate your conda environment\n", "3. Run `conda install py-xgboost`. You must use `conda` to install this package -- currently, it cannot be installed using `pip`  \n", "4. Once installation has completed, run the cell below to verify that everything worked "]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from xgboost import XGBClassifier"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Run the cell below to import everything we'll need for this lab. "]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import pandas as pd\n", "import numpy as np\n", "np.random.seed(0)\n", "import matplotlib.pyplot as plt\n", "from sklearn.model_selection import train_test_split\n", "from sklearn.metrics import accuracy_score\n", "from sklearn.model_selection import GridSearchCV\n", "import warnings\n", "warnings.filterwarnings('ignore')\n", "%matplotlib inline"]}, {"cell_type": "markdown", "metadata": {}, "source": ["The dataset we'll be using for this lab is currently stored in the file `'winequality-red.csv'`.  \n", "\n", "In the cell below, use Pandas to import the dataset into a dataframe, and inspect the `.head()` of the dataframe to ensure everything loaded correctly. "]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["df = None"]}, {"cell_type": "markdown", "metadata": {}, "source": ["For this lab, our target column will be `'quality'`.  That makes this a multiclass classification problem. Given the data in the columns from `'fixed_acidity'` through `'alcohol'`, we'll predict the quality of the wine.  \n", "\n", "This means that we need to store our target variable separately from the dataset, and then split the data and labels into training and test sets that we can use for cross-validation. \n", "\n", "In the cell below:\n", "\n", "- Assign the `'quality'` column to `y` \n", "- Drop this column (`'quality'`) and assign the resulting DataFrame to `X` \n", "- Split the data into training and test sets. Set the `random_state` to 42   "]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["y = None\n", "X = None\n", "\n", "X_train, X_test, y_train, y_test = None"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Now that you have prepared the data for modeling, you can use XGBoost to build a model that can accurately classify wine quality based on the features of the wine!\n", "\n", "The API for xgboost is purposefully written to mirror the same structure as other models in scikit-learn.  "]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Instantiate XGBClassifier\n", "clf = None\n", "\n", "# Fit XGBClassifier\n", "\n", "\n", "# Predict on training and test sets\n", "training_preds = None\n", "test_preds = None\n", "\n", "# Accuracy of training and test sets\n", "training_accuracy = None\n", "test_accuracy = None\n", "\n", "print('Training Accuracy: {:.4}%'.format(training_accuracy * 100))\n", "print('Validation accuracy: {:.4}%'.format(test_accuracy * 100))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Tuning XGBoost\n", "\n", "The model had somewhat lackluster performance on the test set compared to the training set, suggesting the model is beginning to overfit to the training data. Let's tune the model to increase the model performance and prevent overfitting. \n", "\n", "You've already encountered a lot of parameters when working with Decision Trees, Random Forests, and Gradient Boosted Trees.\n", "\n", "For a full list of model parameters, see the [XGBoost Documentation](http://xgboost.readthedocs.io/en/latest/parameter.html).\n", "\n", "Examine the tunable parameters for XGboost, and then fill in appropriate values for the `param_grid` dictionary in the cell below. \n", "\n", "**_NOTE:_** Remember, `GridSearchCV` finds the optimal combination of parameters through an exhaustive combinatoric search.  If you search through too many parameters, the model will take forever to run! To ensure your code runs in sufficient time, we restricted the number of values the parameters can take.  "]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["param_grid = {\n", "    'learning_rate': [0.1, 0.2],\n", "    'max_depth': [6],\n", "    'min_child_weight': [1, 2],\n", "    'subsample': [0.5, 0.7],\n", "    'n_estimators': [100],\n", "}"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Now that we have constructed our `params` dictionary, create a `GridSearchCV` object in the cell below and use it to iterate tune our XGBoost model.  \n", "\n", "Now, in the cell below:\n", "\n", "* Create a `GridSearchCV` object. Pass in the following parameters:\n", "    * `clf`, the classifier\n", "    * `param_grid`, the dictionary of parameters we're going to grid search through\n", "    * `scoring='accuracy'`\n", "    * `cv=None`\n", "    * `n_jobs=1`\n", "* Fit our `grid_clf` object and pass in `X_train` and `y_train`\n", "* Store the best parameter combination found by the grid search in `best_parameters`. You can find these inside the grid search object's `.best_params_` attribute \n", "* Use `grid_clf` to create predictions for the training and test sets, and store them in separate variables \n", "* Compute the accuracy score for the training and test predictions  "]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["grid_clf = None\n", "grid_clf.fit(None, None)\n", "\n", "best_parameters = None\n", "\n", "print('Grid Search found the following optimal parameters: ')\n", "for param_name in sorted(best_parameters.keys()):\n", "    print('%s: %r' % (param_name, best_parameters[param_name]))\n", "\n", "training_preds = None\n", "test_preds = None\n", "training_accuracy = None\n", "test_accuracy = None\n", "\n", "print('')\n", "print('Training Accuracy: {:.4}%'.format(training_accuracy * 100))\n", "print('Validation accuracy: {:.4}%'.format(test_accuracy * 100))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Summary\n", "\n", "Great! You've now successfully made use of one of the most powerful boosting models in data science for modeling.  We've also learned how to tune the model for better performance using the grid search methodology we learned previously. XGBoost is a powerful modeling tool to have in your arsenal. Don't be afraid to experiment with it! "]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.7.3"}}, "nbformat": 4, "nbformat_minor": 2}